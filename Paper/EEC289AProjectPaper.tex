\documentclass[conference]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{cite}

\graphicspath{{figures/}} %set folder for figures

\begin{document}

\title{Anomaly Detection for Time Series Data using VAE-LSTM Model}

\author{
    \IEEEauthorblockN{Randall Fowler}
    \IEEEauthorblockA{\textit{Department of ECE} \\
    \textit{UC Davis}\\
    rlfowler@ucdavis.edu}
    \and
    \IEEEauthorblockN{Conor King}
    \IEEEauthorblockA{\textit{Department of ECE} \\
    \textit{UC Davis}\\
    cfking@ucdavis.edu}
    \and
    \IEEEauthorblockN{Ajay Suresh}
    \IEEEauthorblockA{\textit{Department of ECE} \\
    \textit{UC Davis}\\
    ajsuresh@ucdavis.edu}
}

\maketitle

\begin{abstract}
Here is a abstract...
\end{abstract}

\begin{IEEEkeywords}
Here are some keywords...
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{T}{his} is an introduction...

\section{Methodology}
Anomaly detection is represented by measuring the reconstruction error from a given deep learning model. For this system, a VAE and LSTM hybrid model is used for finding a latent space with a known distribution and capturing temporal information over time series data. This data is presented in a windowed fashion to locate anomalies within a given window. Evaluation of the anomaly detection will be measured using the harmonic mean of precision and recall (F1 score).

\subsection{Preprocessing}
The nature of the data used for this detection system does not need to be specific, but it does require being time series data. With a given window size, a sliding window will sample the original signal, and the VAE input and output will be equivalent to the size of the window. 

As this data does not need to be labeled, it does need to be clean of noise or anomalies. This is due to the model learning the clean distribution of the data, and after training, the poor reconstruction of a signal will indicate the possibility of an anomaly.


\subsection{Variational Autoencoder}
An autoencoder structure establishes a method for compressing information into a dense latent space. This space is not unique as the original signal can be encoded into a large number of different latent spaces. The VAE addresses this non-regularization issue of the latent space by forcing the space to follow a specific distribution\cite{vae_page}. For most VAEs, this distribution is a normal distribution, and the output of the encoder will be a prediction of mean and standard deviation. Once the distribution of the latent space is known, a sampling will be performed to create a latent vector for the input of the decoder. When the latent space distribution is well formed, the output of the decoder aims to reconstruct the original signal as best as possible.

As the autoencoder represents encoder and decoder models, these structures can be created in a variety of ways. For this project, both will be presented by 1-dimensional convolutional layers with a leaky ReLU activation function and batch normalization. The number of samples per window will decrease while depth increases for every layer in the encoder structure. At the end of the encoder, the samples and depth are flattened and passed to linear layers to condense to the final latent space mean and standard deviation values. Samples from the normal distribution will be given to a decoder that is identical to the encoder, but with transpose convolutional layers to increase the size and decrease the depth. Padding for each layer is forced to maintain a consistent  linear decrease in the number of samples. For this architecture, four convolutional layers are used with a split set of linear layers for the mean and standard deviation prediction.

\subsection{Long Short-Term Memory}
With a sampled latent space created from the encoder, the LSTM takes the current latent vector and attempts to predict the next sample. If the VAE is trained well, it should hold the distribution for the dataset, and the LSTM should learn the temporal differences between each latent window. The LSTM tracks previous outcomes by utilizing memory variables or acting as a state machine. Cell memory and hidden state become the long-term and short-term aspects of the model, and those components carry into the model as inputs \cite{lstm_page}. After each iteration, the memory variables will output to be used by another LSTM layer or dropped before other layers.

A standard LSTM layer contains a tangent activation function, and the PyTorch library does not provide an option to adjust the activation function \cite{lstm_pytorch}. For this reason and the lack of activation function post latent space sampling, a linear layer is placed after the final LSTM layer to adjust for scaling.

\subsection{VAE-LSTM Model Training}
Since the two models hold separate functions for the detection system, the training for the VAE and LSTM are handled separately. Both will use the same training set as the models will be combined after training. Batch size, loss function, and optimizer may vary between the two models.

Loss function can differ greatly as the VAE can be trained with Kullback-Leibler (KL) divergence. This loss relates to the similarity between the latent space distribution and the unit normal distribution \cite{vae_page}. Essentially, it enables the latent space to hold this normal distribution shape. Reconstruction loss is another term that focuses on training the weights regardless of the latent space. The sum of the similarity and reconstruction loss promotes a smooth and continuous latent space while improving signal reconstruction.

The LSTM only needs to focus on the error for prediction on known future samples. This error may be the same as reconstruction loss for the VAE. With the memory components, these need to be initialized properly when training. For each given batch, the batch needs to remain in sequential order, and the memory should be reset if batches are not in order. For predictions on an entire dataset, the memory should only be reset at the very beginning. When resetting, the cell memory and hidden state are initialized with a Normal Xavier Initialization \cite{xavier}. This sets the weights using a normal distribution with a mean of 0 and a variance dependent on the number of inputs and outputs of a layer.

\subsection{Anomaly Detection}
Here is the anomaly detection...

\subsection{Evaluation Metrics}
Here are the evaluation metrics...


\section{Experiments}
Talk about design decisions with the model and transition to the datasets used for the experiments.

\subsection{Inertial Measurement Unit Dataset}
Here is the dataset...

\subsection{Synthesized Photoplethysmography Dataset}
Here is the dataset...

\subsection{Electric Vehicle Drive Cycle Dataset}
Here is the dataset...


\section{Results}
Here are the results...

\subsection{Inertial Measurement Unit}
Here is the dataset...

\subsection{Synthesized Photoplethysmography}
Here is the dataset...

\subsection{Electric Vehicle Drive Cycle}
Here is the dataset...


\section{Conclusion}% a little of last page
Here is a conclusion...

\subsection{Future Work}
Here is some future work...

\section*{Acknowledgment}
The authors would like to thank Dr. Yubei Chen for his guidance and support on this course project.

\section*{Contributions}
Randall Fowler developed the models, evaluation methods, and experimented with the IMU dataset. Conor King experimented with the synthesized PPG dataset. Ajay Suresh experimented with the EV drive cycle dataset. All authors contributed to the writing of the paper.

\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}

\bibitem{ref1}
Citation 1

\bibitem{ref2}
Citation 2

\bibitem{ref3}
Citation 3

\bibitem{ref4}
Citation 4

\bibitem{ref5}
Citation 5

\end{thebibliography}

\end{document}